---
# Install Dependencies for DEWrapper Workflow, and perform setup.
- name: Install DEWrapper workflow dependencies
  apt: pkg={{ item }} state=present update_cache=yes cache_valid_time=3600
  with_items:
    - python
    - openjdk-7-jdk

- name: Download get-pip
  get_url: url=https://bootstrap.pypa.io/get-pip.py dest=/tmp/get-pip.py

- name: Install pip
  sudo: True
  shell: python /tmp/get-pip.py

- name: Install docker-py
  sudo: True
  pip: name=docker-py

- name: Install AWS CLI
  sudo: True
  pip: name=awscli

- name: prep aws python module
  sudo: true
  pip: name=boto

- name: Setup aws config directory
  remote_user: '{{ user_name }}' 
  file: state=directory path=/home/{{ user_name }}/.aws

- name: Copy over aws config
  remote_user: '{{ user_name }}'
  copy: src=/home/{{ user_name }}/.aws/config dest=/home/{{ user_name }}/.aws/config owner={{ user_name }}

- name: Copy over aws config again (for boto which is used for ansible's s3 module)
  remote_user: '{{ user_name }}'
  copy: src=/home/{{ user_name }}/.aws/credentials dest=/home/{{ user_name }}/.aws/credentials owner={{ user_name }}

- name: Copy over aws config again (for boto which is used for ansible's s3 module)
  remote_user: '{{ user_name }}'
  copy: src=/home/{{ user_name }}/.aws/credentials dest=/home/{{ user_name }}/.boto owner={{ user_name }}

# ansible s3 module uses boto to connect to aws, and boto uses a .boto config file which is the 
# same as the aws_credentials file, except the header says "[Credentials]".
- name: Update .boto file
  remote_user: '{{ user_name }}'
  lineinfile: dest=/home/{{ user_name }}/.boto regexp="\[([^\[\]]*)\]" line="[Credentials]" backrefs=yes

  # The s3 module does not support multipart files. If you try to use s3 module to get these files, you get this error:
  # "Files uploaded with multipart of s3 are not supported with checksum, unable to compute checksum."
  # So, the result is that we must use the shell module to call the aws s3 command to download multipart files.
#- name: Copy over DKFZ image 
#  sudo_user: '{{ user_name }}'
#  s3: bucket=oicr.docker.private.images object=dkfz_dockered_workflows.tar dest=/home/{{ user_name }}/dkfz_dockered_workflows.tar mode=get

- name: Copy over DKFZ image 
  sudo_user: '{{ user_name }}'
  shell: creates=/home/{{ user_name }}/dkfz_dockered_workflows.tar aws s3 cp s3://oicr.docker.private.images/dkfz_dockered_workflows.tar /home/{{ user_name }}/dkfz_dockered_workflows.tar 

  #NOTE: Not sure, but it seems to me that Ansible unarchive module prefers .tgz to .tar.gz
- name: Copy over bundledFiles supporting DKFZ container
  sudo_user: '{{ user_name }}'
  shell: creates=/home/{{ user_name }}/dkfz-workflow-dependencies_150218_0931.tgz aws s3 cp s3://oicr.docker.private.images/dkfz-workflow-dependencies_150218_0931.tar.gz /home/{{ user_name }}/dkfz-workflow-dependencies_150218_0931.tgz

- name: Get docker container seqware_whitestar
  docker:
    image: seqware/seqware_whitestar

- name: Get docker container seqware_full
  docker:
    image: seqware/seqware_full

- name: Get docker container pcawg-delly-workflow
  docker:
    image: pancancer/pcawg-delly-workflow

- name: Load DKFZ image
  shell: docker load -i /home/{{ user_name }}/dkfz_dockered_workflows.tar
  
- name: Setup datastore
  sudo_user: '{{ user_name }}'
  file: path=~/bundledFiles state=directory

# Before running unarchive, check the the file isn't already unarchived: the unarchive module will take a very long time to run,
# even if the file is already unarchived.
- name: check if already unarchived
  stat: path=~/bundledFiles
  sudo_user: '{{ user_name }}'
  register: bundledFilesStat
 
- name: Untar bundledFiles
  sudo_user: '{{ user_name }}'
  unarchive: src=~/dkfz-workflow-dependencies_150218_0931.tgz dest=~/bundledFiles copy=no
  when: not bundledFilesStat.stat.exists

- name: Create workflows directory
  sudo: yes
  file: path=/workflows state=directory owner=ubuntu group=ubuntu mode=0777

- name: check if /datastore already exists
  stat: path=/datastore
  register: datastoreStat

# This may already exists (created in another playbook), as symlink to /mnt/datastore
- name: Create datastore directory
  sudo: yes
  file: path=/datastore state=directory owner=ubuntu group=ubuntu mode=0777
  when: not datastoreStat.stat.exists

- name: Get Seqware artifact
  sudo_user: '{{ user_name }}'
  get_url: dest=~/seqware-distribution-1.1.0-alpha.6-full.jar url=https://seqwaremaven.oicr.on.ca/artifactory/seqware-release/com/github/seqware/seqware-distribution/1.1.0-alpha.6/seqware-distribution-1.1.0-alpha.6-full.jar

- name: Unzip DEWrapperWorkflow
  sudo_user: '{{ user_name }}'
  shell: java -cp ~/seqware-distribution-1.1.0-alpha.6-full.jar net.sourceforge.seqware.pipeline.tools.UnZip --input-zip Workflow_Bundle_DEWrapperWorkflow_1.0-SNAPSHOT_SeqWare_1.1.0-rc.1.zip --output-dir Workflow_Bundle_DEWrapperWorkflow_1.0-SNAPSHOT_SeqWare_1.1.0-rc.1

- name: Get workflow launcher script
  sudo_user: '{{ user_name }}'
  get_url: dest=~/launchWorkflow.sh url=https://raw.githubusercontent.com/SeqWare/public-workflows/develop/DEWrapperWorkflow/launchWorkflow.sh

- name: Get workflow launcher script for Whitestar
  sudo_user: '{{ user_name }}'
  get_url: dest=~/launchWorkflowDev.sh url=https://raw.githubusercontent.com/SeqWare/public-workflows/develop/DEWrapperWorkflow/launchWorkflowDev.sh
